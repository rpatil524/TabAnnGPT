{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "# setting path\n",
    "sys.path.append('..')\n",
    "from utils import save_pickle_file, clean_text, load_txt_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57335ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gts\n",
    "cta_train_gt = pd.read_csv(\"data/sets/limaye_cta_train.csv\")\n",
    "cta_test_gt = pd.read_csv(\"data/sets/limaye_cta_test.csv\")\n",
    "cta_train_gt.fillna('', inplace=True)\n",
    "cta_test_gt.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4502ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {'train':{}, 'test':{}}\n",
    "for index, row in cta_train_gt.iterrows():\n",
    "    if row[\"file_name\"] not in gt['train']:\n",
    "        gt['train'][row[\"file_name\"]] = {}\n",
    "    gt['train'][row[\"file_name\"]][row[\"col_index\"]] = [ [row[\"label\"]] if row[\"label_2\"] == \"\" else [row[\"label\"], row[\"label_2\"]], row[\"label_2\"],row[\"all_labels\"]]\n",
    "    \n",
    "for index, row in cta_test_gt.iterrows():\n",
    "    if row[\"file_name\"] not in gt['test']:\n",
    "        gt['test'][row[\"file_name\"]] = {}\n",
    "    gt['test'][row[\"file_name\"]][row[\"col_index\"]] = [ [row[\"label\"]] if row[\"label_2\"] == \"\" else [row[\"label\"], row[\"label_2\"]], row[\"label_2\"],row[\"all_labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Limaye dataset tables\n",
    "# Tables can be downloaded at: https://github.com/alan-turing-institute/SemAIDA/tree/master/IJCAI19/SemColHNN_Codes/Limaye\n",
    "table_path = \"../../../Benchmarks/Limaye/tables_instance/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = load_txt_file(f\"data/limayeu-labels/limayeu_all_labels.txt\")\n",
    "labels_to_text = {label: label for label in all_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f7733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "\n",
    "for _, table_row in cta_train_gt.iterrows():\n",
    "    \n",
    "    table_df = pd.read_csv(f\"{table_path}{table_row['file_name']}.csv\",header=None)\n",
    "    table_df = table_df.dropna(how='all')\n",
    "    ordered_labels = []\n",
    "    ordered_types = []\n",
    "    \n",
    "    for i, c in enumerate(table_df.columns):\n",
    "        if i in gt[\"train\"][table_row[\"file_name\"]]:\n",
    "            ordered_labels.append(gt[\"train\"][table_row[\"file_name\"]][i][0])\n",
    "            ordered_types.append(gt[\"train\"][table_row[\"file_name\"]][i][1]) # actually second label\n",
    "        else: # Unlabeled columns added for context\n",
    "            if len(table_df[[i]].dropna(how=\"all\")):\n",
    "                ordered_labels.append(\"\")\n",
    "                ordered_types.append(\"\")\n",
    "\n",
    "\n",
    "    cleaned_columns = []\n",
    "\n",
    "    for i, c in enumerate(table_df.columns):\n",
    "        if i in gt[\"train\"][table_row[\"file_name\"]]:\n",
    "            cleaned_rows = []\n",
    "            for row in table_df.iloc[:, i].tolist():\n",
    "                cleaned = \" \".join(clean_text(row).split()[:20]) #select 20 words\n",
    "                if cleaned != \"\":\n",
    "                    cleaned_rows.append(cleaned)\n",
    "            cleaned_columns.append(cleaned_rows)\n",
    "        else:\n",
    "            # Unlabeled columns added for context\n",
    "            if len(table_df[[i]].dropna(how=\"all\")):\n",
    "                cleaned_rows = []\n",
    "                for row in table_df.iloc[:, i].tolist():\n",
    "                    cleaned = \" \".join(clean_text(row).split()[:20]) #select 20 words\n",
    "                    if cleaned != \"\":\n",
    "                        cleaned_rows.append(cleaned)\n",
    "                cleaned_columns.append(cleaned_rows)\n",
    "\n",
    "    # Add empty if not len 5\n",
    "    for i, col_rows in enumerate(cleaned_columns):\n",
    "        if len(col_rows) < 5: # number of rows\n",
    "            for j in range(5-len(col_rows)):\n",
    "                cleaned_columns[i].append(\"\")\n",
    "\n",
    "    table_list_df = []\n",
    "\n",
    "    # Create table list for dataframe\n",
    "    for j in range(5):\n",
    "        new_row = []\n",
    "        for cleaned_column in cleaned_columns:\n",
    "            new_row.append(cleaned_column[j])\n",
    "        table_list_df.append(new_row)\n",
    "\n",
    "    # Markdown format\n",
    "    df_new = pd.DataFrame(table_list_df, columns=[ f\"Column {i+1}\" for i in range(len(cleaned_columns))])\n",
    "    # uncomment for knowledge generation set (-kg):\n",
    "    # df_new = pd.DataFrame(table_list_df, columns=[\", \".join([labels_to_text[m] for m in ordered_labels[i] ]) for i in range(len(cleaned_columns))])\n",
    "\n",
    "    table_string = df_new.to_markdown(index=False)\n",
    "    column_names = [ f\"Column {i+1}\" for i in range(len(cleaned_columns))]\n",
    "        \n",
    "    train_examples.append([table_row[\"file_name\"], table_string, ordered_labels, \"\", ordered_types, column_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13159d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = []\n",
    "\n",
    "for _, table_row in cta_test_gt.iterrows():\n",
    "    \n",
    "    table_df = pd.read_csv(f\"{table_path}{table_row['file_name']}.csv\",header=None)\n",
    "    table_df = table_df.dropna(how='all')\n",
    "    ordered_labels = []\n",
    "    ordered_types = []\n",
    "    \n",
    "    for i, c in enumerate(table_df.columns):\n",
    "        if i in gt[\"test\"][table_row[\"file_name\"]]:\n",
    "            ordered_labels.append(gt[\"test\"][table_row[\"file_name\"]][i][0])\n",
    "            ordered_types.append(gt[\"test\"][table_row[\"file_name\"]][i][1]) # actually second label\n",
    "        else:\n",
    "            if len(table_df[[i]].dropna(how=\"all\")):\n",
    "                ordered_labels.append(\"\")\n",
    "                ordered_types.append(\"\")\n",
    "\n",
    "    cleaned_columns = []\n",
    "\n",
    "    for i, c in enumerate(table_df.columns):\n",
    "        if i in gt[\"test\"][table_row[\"file_name\"]]:\n",
    "            cleaned_rows = []\n",
    "            for row in table_df.iloc[:, i].tolist():\n",
    "                cleaned = \" \".join(clean_text(row).split()[:20]) #select 20 words\n",
    "                if cleaned != \"\":\n",
    "                    cleaned_rows.append(cleaned)\n",
    "            cleaned_columns.append(cleaned_rows)\n",
    "        else:\n",
    "            # Unlabeled column\n",
    "            if len(table_df[[i]].dropna(how=\"all\")):\n",
    "                cleaned_rows = []\n",
    "                for row in table_df.iloc[:, i].tolist():\n",
    "                    cleaned = \" \".join(clean_text(row).split()[:20]) #select 20 words\n",
    "                    if cleaned != \"\":\n",
    "                        cleaned_rows.append(cleaned)\n",
    "                cleaned_columns.append(cleaned_rows)\n",
    "\n",
    "    # Add empty if not len 5\n",
    "    for i, col_rows in enumerate(cleaned_columns):\n",
    "        if len(col_rows) < 5: # number of rows\n",
    "            for j in range(5-len(col_rows)):\n",
    "                cleaned_columns[i].append(\"\")\n",
    "\n",
    "    table_list_df = []\n",
    "\n",
    "    # Create table list for dataframe\n",
    "    for j in range(5):\n",
    "        new_row = []\n",
    "        for cleaned_column in cleaned_columns:\n",
    "            new_row.append(cleaned_column[j])\n",
    "        table_list_df.append(new_row)\n",
    "\n",
    "    # Markdown format\n",
    "    # Create the new dataframe\n",
    "    df_new = pd.DataFrame(table_list_df, columns=[ f\"Column {i+1}\" for i in range(len(cleaned_columns))])\n",
    "    # uncomment for knowledge generation set (-kg):\n",
    "    # df_new = pd.DataFrame(table_list_df, columns=[labels_to_text[ordered_labels[i]] for i in range(len(cleaned_columns))])\n",
    "\n",
    "    table_string = df_new.to_markdown(index=False)\n",
    "    column_names = [ f\"Column {i+1}\" for i in range(len(cleaned_columns))]\n",
    "        \n",
    "    test_examples.append([table_row[\"file_name\"], table_string, ordered_labels, \"\", ordered_types, column_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c60c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 105, Test examples: 107\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train examples: {len(train_examples)}, Test examples: {len(test_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_file(\"../data/limayeu-cta-train.pkl\",train_examples)\n",
    "save_pickle_file(\"../data/limayeu-cta-test.pkl\",test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf405b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_file(\"../data/limayeu-cta-train-kg.pkl\",train_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
