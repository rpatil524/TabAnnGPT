{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed23b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from utils import load_pickle_file, save_pickle_file, map_answers_column, calculate_f1_scores, decimal, load_cta_dataset_column\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\" #4,5,6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"sotabv2\", \"t2dv2-webtables\", \"sportstables\"]\n",
    "\n",
    "# StableBeluga7B\n",
    "model_name = \"stabilityai/StableBeluga-7B\"\n",
    "mod = \"stablebeluga7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"hf_cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", cache_dir=\"hf_cache/\")\n",
    "\n",
    "# SOLAR\n",
    "# model_name = \"upstage/SOLAR-0-70b-16bit\"\n",
    "# mod = \"solar\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"hf_cache/\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, load_in_8bit=True, device_map=\"auto\", cache_dir=\"hf_cache/\", temperature=0, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_template = \"\"\"Answer the question based on the task, instructions and definitions below.\n",
    "Definitions: \n",
    "{definitions}\n",
    "Task: Classify the column given to you into only one of these classes that are separated with comma: {labels_joined}.\n",
    "Instructions: 1. Look at the column and the classes given to you. 2. Examine the values of the column. 3. Select a class that best represents the meaning of the column. 4. Answer with the selected class.\n",
    "Column: {input_string}\n",
    "Class:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4e3d1",
   "metadata": {},
   "source": [
    "## Manual definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    examples, labels, train_examples, train_labels, labels_to_text, text_to_label, labels_joined, train, test = load_cta_dataset_column(dataset,\"\")\n",
    "\n",
    "    f = open(f'../data/{dataset}-definitions.txt')\n",
    "    definitions = json.load(f)\n",
    "    all_labels = [labels_to_text[defn] for defn in definitions]\n",
    "    definitions = [definitions[defn] for defn in definitions]\n",
    "    test_embeddings = load_pickle_file(f'embeddings/test_embeddings_{dataset}-column.pkl')\n",
    "    \n",
    "    prompt = PromptTemplate(template=zero_template, input_variables=['input_string', 'labels_joined', 'definitions'])\n",
    "\n",
    "    # Pick the necessary deinitions for each example\n",
    "    knowledge_embeddings = load_pickle_file(f\"embeddings/{dataset}-definitions-embeddings.pkl\")\n",
    "    examples_demonstrations = []\n",
    "    for i, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "        cos = cosine_similarity([test_embeddings[i]], knowledge_embeddings)\n",
    "        cos_dict = {}\n",
    "        for j, c in enumerate(cos[0]):\n",
    "            cos_dict[j] = c\n",
    "        sorted_cos_dict = {k: v for k, v in sorted(cos_dict.items(), key=lambda item: item[1])}\n",
    "        examples_demonstrations.append(list(sorted_cos_dict.keys())[-10:])\n",
    "\n",
    "    prompts = []\n",
    "    model_answers = []\n",
    "\n",
    "    for j, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "        \n",
    "        try:\n",
    "            definitions_string = \"\"\"\"\"\"\n",
    "            for i in examples_demonstrations[j][-5:]:\n",
    "                definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "            definitions_string = definitions_string.strip()\n",
    "\n",
    "            text_prompt = prompt.format(input_string=example.strip(), labels_joined=labels_joined, definitions=definitions_string)\n",
    "            prompts.append(text_prompt)\n",
    "            \n",
    "            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=100)\n",
    "            model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "        except Exception:\n",
    "            definitions_string = \"\"\"\"\"\"\n",
    "            for i in examples_demonstrations[j][-3:]:\n",
    "                definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "            definitions_string = definitions_string.strip()\n",
    "\n",
    "            text_prompt = prompt.format(input_string=example.strip(), labels_joined=labels_joined, definitions=definitions_string)\n",
    "            prompts.append(text_prompt)\n",
    "\n",
    "            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=100)\n",
    "            model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/manual-definitions-prompt-column-0-shot.pkl\", model_answers)\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/manual-definitions-prompt-column-0-shot-prompts.pkl\", prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7e22f",
   "metadata": {},
   "source": [
    "## LLM Definitions experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b78bfff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_prompts = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\"]\n",
    "tb_prompts = [\"TB1\", \"TB2\", \"TB3\", \"TB4\", \"TB5\", \"TB6\", \"TB7\"]\n",
    "inst_prompts = [\"I1\", \"I2\"]\n",
    "syst_prompts = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    examples, labels, train_examples, train_labels, labels_to_text, text_to_label, labels_joined, train, test = load_cta_dataset_column(dataset,\"\")\n",
    "    all_labels = [labels_to_text[l] for l in labels_to_text]\n",
    "    \n",
    "    test_embeddings = load_pickle_file(f\"embeddings/test_embeddings_{dataset}-column.pkl\")\n",
    "    \n",
    "    for syst in syst_prompts:\n",
    "        for g in a_prompts:\n",
    "            if f\"{syst}_{g}-column-0-shot.pkl\" not in os.listdir(f\"predictions/{dataset}/{mod}/\"):\n",
    "                if f\"{syst}_{g}_prompt_knowledge.pkl\" in os.listdir(f\"knowledge/{mod}/{dataset}/\"):\n",
    "                    print(f\"Loading knowledge {syst}_{g}\")\n",
    "                    definitions = load_pickle_file(f\"knowledge/{mod}/{dataset}/{syst}_{g}_prompt_knowledge.pkl\")\n",
    "                    prompts = load_pickle_file(f\"knowledge/{mod}/{dataset}/{syst}_{g}_prompt_knowledge-prompts.pkl\")\n",
    "                    definitions = [d.replace(prompts[i],\"\").strip() for i, d in enumerate(definitions)]\n",
    "                    knowledge_embeddings = load_pickle_file(f\"embeddings/{mod}/{syst}_{g}_knowledge_embeddings_{dataset}.pkl\")\n",
    "                    \n",
    "                    examples_demonstrations = []\n",
    "                    for i, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "                        cos = cosine_similarity([test_embeddings[i]], knowledge_embeddings)\n",
    "                        cos_dict = {}\n",
    "                        for j, c in enumerate(cos[0]):\n",
    "                            cos_dict[j] = c\n",
    "                        sorted_cos_dict = {k: v for k, v in sorted(cos_dict.items(), key=lambda item: item[1])}\n",
    "                        examples_demonstrations.append(list(sorted_cos_dict.keys())[-10:])\n",
    "\n",
    "                    prompt = PromptTemplate(template=zero_template, input_variables=['input_string', 'labels_joined', 'definitions'])\n",
    "                    prompts = []\n",
    "                    model_answers = []\n",
    "\n",
    "                    for j, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "\n",
    "                        try:\n",
    "                            definitions_string = \"\"\"\"\"\"\n",
    "                            for i in examples_demonstrations[j][-5:]:\n",
    "                                definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "                            definitions_string = definitions_string.strip()\n",
    "\n",
    "                            text_prompt = prompt.format(input_string=example.strip(), labels_joined=labels_joined, definitions=definitions_string)\n",
    "\n",
    "                            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "                            prompts.append(text_prompt)\n",
    "                            model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "                        except Exception:\n",
    "                            definitions_string = \"\"\"\"\"\"\n",
    "                            for i in examples_demonstrations[j][-3:]:\n",
    "                                definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "                            definitions_string = definitions_string.strip()\n",
    "\n",
    "                            text_prompt = prompt.format(input_string=example.strip(), labels_joined=labels_joined, definitions=definitions_string)\n",
    "\n",
    "                            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "                            prompts.append(text_prompt)\n",
    "                            model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "                    save_pickle_file(f\"predictions/{dataset}/{mod}/{syst}_{g}-column-0-shot.pkl\", model_answers)\n",
    "                    save_pickle_file(f\"predictions/{dataset}/{mod}/{syst}_{g}-column-0-shot-prompts.pkl\", prompts)\n",
    "\n",
    "    # Run B prompts             \n",
    "    for tb in tb_prompts:\n",
    "        for syst in syst_prompts[:-1]: # skip last system for prompt b\n",
    "            for inst in inst_prompts:\n",
    "                if f\"{syst}_{inst}_{tb}-column-0-shot.pkl\" not in os.listdir(f\"predictions/{dataset}/{mod}/\") and f\"{syst}_{inst}_{tb}_prompt_knowledge.pkl\" in os.listdir(f\"knowledge/{mod}/{dataset}/\"):\n",
    "                    if f\"{syst}_{inst}_{tb}_prompt_knowledge.pkl\" in os.listdir(f\"knowledge/{mod}/{dataset}/\"):\n",
    "                        print(f\"Loading knowledge {tb}_{syst}_{inst}\")\n",
    "                        definitions = load_pickle_file(f\"knowledge/{mod}/{dataset}/{syst}_{inst}_{tb}_prompt_knowledge.pkl\")\n",
    "                        prompts = load_pickle_file(f\"knowledge/{mod}/{dataset}/{syst}_{inst}_{tb}_prompt_knowledge-prompts.pkl\")\n",
    "                        definitions = [d.replace(prompts[i],\"\").strip() for i, d in enumerate(definitions)]\n",
    "                        knowledge_embeddings = load_pickle_file(f\"embeddings/{mod}/{syst}_{inst}_{tb}_knowledge_embeddings_{dataset}.pkl\")\n",
    "\n",
    "                        examples_demonstrations = []\n",
    "                        for i, example in enumerate(examples):\n",
    "                            cos = cosine_similarity([test_embeddings[i]], knowledge_embeddings)\n",
    "                            cos_dict = {}\n",
    "                            for j, c in enumerate(cos[0]):\n",
    "                                cos_dict[j] = c\n",
    "                            sorted_cos_dict = {k: v for k, v in sorted(cos_dict.items(), key=lambda item: item[1])}\n",
    "                            examples_demonstrations.append(list(sorted_cos_dict.keys())[-10:])\n",
    "\n",
    "                        prompt = PromptTemplate(template=zero_template, input_variables=['input_string', 'labels_joined', 'definitions'])\n",
    "                        prompts = []\n",
    "                        model_answers = []\n",
    "\n",
    "                        for j, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "\n",
    "                            try:\n",
    "                                definitions_string = \"\"\"\"\"\"\n",
    "                                for i in examples_demonstrations[j][-5:]:\n",
    "                                    definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "                                definitions_string = definitions_string.strip()\n",
    "\n",
    "                                text_prompt = prompt.format(input_string=example.strip(), labels_joined=labels_joined, definitions=definitions_string)\n",
    "\n",
    "                                inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                                output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "                                prompts.append(text_prompt)\n",
    "                                model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "                            except Exception:\n",
    "                                definitions_string = \"\"\"\"\"\"\n",
    "                                for i in examples_demonstrations[j][-5:]:\n",
    "                                    definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "                                definitions_string = definitions_string.strip()\n",
    "\n",
    "                                text_prompt = prompt.format(input_string=example.strip(), labels_joined=labels_joined, definitions=definitions_string)\n",
    "\n",
    "                                inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                                output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "                                prompts.append(text_prompt)\n",
    "                                model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "                        save_pickle_file(f\"predictions/{dataset}/{mod}/{syst}_{inst}_{tb}-column-0-shot.pkl\", model_answers)\n",
    "                        save_pickle_file(f\"predictions/{dataset}/{mod}/{syst}_{inst}_{tb}-column-0-shot-prompts.pkl\", prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e8620",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b83179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for syst in syst_prompts:\n",
    "    for g in a_prompts:\n",
    "        if f\"{syst}_{g}-column-0-shot.pkl\" in os.listdir(f\"predictions/{dataset}/{model}/\"):\n",
    "            preds = load_pickle_file(f\"predictions/{dataset}/{model}/{syst}_{g}-column-0-shot.pkl\")\n",
    "            prompts = load_pickle_file(f\"predictions/{dataset}/{model}/{syst}_{g}-column-0-shot-prompts.pkl\")\n",
    "\n",
    "            predictions, num = map_answers_column(preds,prompts)\n",
    "\n",
    "            types = list(set(labels))\n",
    "            types = types + [\"-\"] if \"-\" in predictions else types\n",
    "            evaluation, per_class_eval = calculate_f1_scores(labels, predictions, len(types), types)\n",
    "\n",
    "            print(f\"{syst}_{g}\\t{decimal(evaluation['Precision'])}\\t{decimal(evaluation['Recall'])}\\t{decimal(evaluation['Macro-F1'])}\\t{decimal(evaluation['Micro-F1'])}\\t{num}\")  \n",
    "\n",
    "for tb in tb_prompts:\n",
    "    for syst in syst_prompts[:-1]: # skip last system for prompt b\n",
    "        for inst in inst_prompts:\n",
    "            if f\"{syst}_{inst}_{tb}-column-0-shot.pkl\" in os.listdir(f\"predictions/{dataset}/{model}/\"):\n",
    "                preds = load_pickle_file(f\"predictions/{dataset}/{model}/{syst}_{inst}_{tb}-column-0-shot.pkl\")\n",
    "                prompts = load_pickle_file(f\"predictions/{dataset}/{model}/{syst}_{inst}_{tb}-column-0-shot-prompts.pkl\")\n",
    "\n",
    "                predictions, num = map_answers_column(preds,prompts)\n",
    "\n",
    "                types = list(set(labels))\n",
    "                types = types + [\"-\"] if \"-\" in predictions else types\n",
    "                evaluation, per_class_eval = calculate_f1_scores(labels, predictions, len(types), types)\n",
    "\n",
    "                print(f\"{syst}_{inst}_{tb}\\t{decimal(evaluation['Precision'])}\\t{decimal(evaluation['Recall'])}\\t{decimal(evaluation['Macro-F1'])}\\t{decimal(evaluation['Micro-F1'])}\\t{num}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2dae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
