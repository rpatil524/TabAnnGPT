{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed23b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import json\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from utils import load_cpa_dataset_column, save_pickle_file, load_pickle_file, map_answers_column, decimal, calculate_f1_scores\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"#3,4,5,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15be850",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"sotabv2\", \"t2dv2-webtables\"]\n",
    "\n",
    "# StableBeluga7B\n",
    "model_name = \"stabilityai/StableBeluga-7B\"\n",
    "mod = \"stablebeluga7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"hf_cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", cache_dir=\"hf_cache/\")\n",
    "\n",
    "# SOLAR\n",
    "# model_name = \"upstage/SOLAR-0-70b-16bit\"\n",
    "# mod = \"solar\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"hf_cache/\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, load_in_8bit=True, device_map=\"auto\", cache_dir=\"hf_cache/\", temperature=0, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123db3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_prompts = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\"]\n",
    "tb_prompts = [\"TB1\", \"TB2\", \"TB3\", \"TB4\", \"TB5\", \"TB6\", \"TB7\"]\n",
    "inst_prompts = [\"I1\", \"I2\"]\n",
    "syst_prompts = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ef568",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the task, instructions and definitions below.\n",
    "Definitions:\n",
    "{definitions}\n",
    "Task: Classify the relationship between two columns with one of the following classes that are separated with comma: {labels_joined}.\n",
    "Instructions: 1. Look at the two columns and the classes given to you 2. Look at their values in detail. 3. Select a class that best represents the relationship between the two columns. 4. Answer with only one class.\n",
    "Column1: {column_1}\n",
    "Column2: {column_2}\n",
    "Class:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    examples, labels, train_examples, train_labels, labels_to_text = load_cpa_dataset_column(dataset,\"\")\n",
    "    labels_joined = \", \".join([labels_to_text[l] for l in labels_to_text])\n",
    "    all_labels = [labels_to_text[l] for l in labels_to_text]\n",
    "\n",
    "    test_embeddings = load_pickle_file(f\"embeddings/cpa-test_embeddings_{dataset}-column.pkl\")\n",
    "\n",
    "    # Test A prompts\n",
    "    for syst in syst_prompts:\n",
    "        for g in a_prompts:\n",
    "            print(f\"Loading knowledge {syst}_{g}\")\n",
    "            definitions = load_pickle_file(f\"knowledge/{mod}/{dataset}/cpa-{syst}_{g}_prompt_knowledge.pkl\")\n",
    "            prompts = load_pickle_file(f\"knowledge/{mod}/{dataset}/cpa-{syst}_{g}_prompt_knowledge-prompts.pkl\")\n",
    "            definitions = [d.replace(prompts[i],\"\").strip() for i, d in enumerate(definitions)]\n",
    "            knowledge_embeddings = load_pickle_file(f\"embeddings/{mod}/cpa-{syst}_{g}_knowledge_embeddings_{dataset}.pkl\")\n",
    "            \n",
    "            examples_demonstrations = []\n",
    "            for i, example in enumerate(examples):\n",
    "                cos = cosine_similarity([test_embeddings[i]], knowledge_embeddings)\n",
    "                cos_dict = {}\n",
    "                for j, c in enumerate(cos[0]):\n",
    "                    cos_dict[j] = c\n",
    "                sorted_cos_dict = {k: v for k, v in sorted(cos_dict.items(), key=lambda item: item[1])}\n",
    "                examples_demonstrations.append(list(sorted_cos_dict.keys())[-10:])\n",
    "\n",
    "            prompt = PromptTemplate(template=template, input_variables=['labels_joined', 'definitions', 'column_1', 'column_2'])\n",
    "            prompts = []\n",
    "            model_answers = []\n",
    "\n",
    "            for j, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "\n",
    "                try:\n",
    "                    definitions_string = \"\"\"\"\"\"\n",
    "                    for i in examples_demonstrations[j]:\n",
    "                        definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "                    definitions_string = definitions_string.strip()\n",
    "\n",
    "                    text_prompt = prompt.format(labels_joined=labels_joined, definitions=definitions_string, column_1=example[0], column_2=example[1])\n",
    "\n",
    "                    inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                    output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "                    prompts.append(text_prompt)\n",
    "                    model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "                except Exception:\n",
    "                    definitions_string = \"\"\"\"\"\"\n",
    "                    for i in examples_demonstrations[j][-5:]:\n",
    "                        definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "                    definitions_string = definitions_string.strip()\n",
    "\n",
    "                    text_prompt = prompt.format(labels_joined=labels_joined, definitions=definitions_string, column_1=example[0], column_2=example[1])\n",
    "\n",
    "                    inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                    output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "                    prompts.append(text_prompt)\n",
    "                    model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "            save_pickle_file(f\"predictions/{dataset}/{mod}/cpa-{syst}_{g}-column-0-shot.pkl\", model_answers)\n",
    "            save_pickle_file(f\"predictions/{dataset}/{mod}/cpa-{syst}_{g}-column-0-shot-prompts.pkl\", prompts)\n",
    "\n",
    "    # Run B prompts\n",
    "    for tb in tb_prompts:\n",
    "        for syst in syst_prompts[:-1]: # skip last system for prompt b\n",
    "            for inst in inst_prompts:\n",
    "                print(f\"Loading knowledge {tb}_{syst}_{inst}\")\n",
    "                definitions = load_pickle_file(f\"knowledge/{model_name}/{dataset}/cpa-{syst}_{inst}_{tb}_prompt_knowledge.pkl\")\n",
    "                prompts = load_pickle_file(f\"knowledge/{mod}/{dataset}/cpa-{syst}_{g}_prompt_knowledge-prompts.pkl\")\n",
    "                definitions = [d.replace(prompts[i],\"\").strip() for i, d in enumerate(definitions)]\n",
    "                knowledge_embeddings = load_pickle_file(f\"embeddings/{model_name}/cpa-{syst}_{inst}_{tb}_knowledge_embeddings_{dataset}.pkl\")\n",
    "\n",
    "                examples_demonstrations = []\n",
    "                for i, example in enumerate(examples):\n",
    "                    cos = cosine_similarity([test_embeddings[i]], knowledge_embeddings)\n",
    "                    cos_dict = {}\n",
    "                    for j, c in enumerate(cos[0]):\n",
    "                        cos_dict[j] = c\n",
    "                    sorted_cos_dict = {k: v for k, v in sorted(cos_dict.items(), key=lambda item: item[1])}\n",
    "                    examples_demonstrations.append(list(sorted_cos_dict.keys())[-10:])\n",
    "\n",
    "                prompt = PromptTemplate(template=template, input_variables=['labels_joined', 'definitions', 'column_1', 'column_2'])\n",
    "                prompts = []\n",
    "                model_answers = []\n",
    "\n",
    "                for j, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "                    try:\n",
    "                        definitions_string = \"\"\"\"\"\"\n",
    "                        for i in examples_demonstrations[j]:\n",
    "                            definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "                        definitions_string = definitions_string.strip()\n",
    "\n",
    "                        text_prompt = prompt.format(labels_joined=labels_joined, definitions=definitions_string, column_1=example[0], column_2=example[1])\n",
    "\n",
    "                        inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                        output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "                        prompts.append(text_prompt)\n",
    "                        model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "                    except Exception:\n",
    "                        definitions_string = \"\"\"\"\"\"\n",
    "                        for i in examples_demonstrations[j][-5:]:\n",
    "                            definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "                        definitions_string = definitions_string.strip()\n",
    "\n",
    "                        text_prompt = prompt.format(labels_joined=labels_joined, definitions=definitions_string, column_1=example[0], column_2=example[1])\n",
    "\n",
    "                        inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                        output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "                        prompts.append(text_prompt)\n",
    "                        model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "                save_pickle_file(f\"predictions/{dataset}/{mod}/cpa-{syst}_{inst}_{tb}-column-0-shot.pkl\", model_answers)\n",
    "                save_pickle_file(f\"predictions/{dataset}/{mod}/cpa-{syst}_{inst}_{tb}-column-0-shot-prompts.pkl\", prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35fe99",
   "metadata": {},
   "source": [
    "## Manual definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    examples, labels, train_examples, train_labels, labels_to_text = load_cpa_dataset_column(dataset,\"\")\n",
    "    labels_joined = \", \".join([labels_to_text[l] for l in labels_to_text])\n",
    "    \n",
    "    f = open(f'data/cpa-{dataset}-definitions.txt')\n",
    "    definitions = json.load(f)\n",
    "    all_labels = [labels_to_text[defn] for defn in definitions]\n",
    "    definitions = [definitions[defn] for defn in definitions]\n",
    "    test_embeddings = load_pickle_file(f'embeddings/cpa-test_embeddings_{dataset}-column.pkl')\n",
    "    \n",
    "    prompt = PromptTemplate(template=template, input_variables=['labels_joined', 'definitions', 'column_1', 'column_2'])\n",
    "\n",
    "    # Pick the necessary deinitions for each example\n",
    "    knowledge_embeddings = load_pickle_file(f\"embeddings/cpa-{dataset}-definitions-embeddings.pkl\")\n",
    "    examples_demonstrations = []\n",
    "    for i, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "        cos = cosine_similarity([test_embeddings[i]], knowledge_embeddings)\n",
    "        cos_dict = {}\n",
    "        for j, c in enumerate(cos[0]):\n",
    "            cos_dict[j] = c\n",
    "        sorted_cos_dict = {k: v for k, v in sorted(cos_dict.items(), key=lambda item: item[1])}\n",
    "        examples_demonstrations.append(list(sorted_cos_dict.keys())[-10:])\n",
    "\n",
    "    prompts = []\n",
    "    model_answers = []\n",
    "\n",
    "    for j, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "        \n",
    "        try:\n",
    "            definitions_string = \"\"\"\"\"\"\n",
    "            for i in examples_demonstrations[j][-5:]:\n",
    "                definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "            definitions_string = definitions_string.strip()\n",
    "\n",
    "            text_prompt = prompt.format(labels_joined=labels_joined, definitions=definitions_string, column_1=example[0], column_2=example[1])\n",
    "\n",
    "            prompts.append(text_prompt)\n",
    "            \n",
    "            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=100)\n",
    "            model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "        except Exception:\n",
    "            definitions_string = \"\"\"\"\"\"\n",
    "            for i in examples_demonstrations[j][-3:]:\n",
    "                definitions_string += f\"{all_labels[i]}: {definitions[i]}\\n\"\n",
    "            definitions_string = definitions_string.strip()\n",
    "\n",
    "            text_prompt = prompt.format(labels_joined=labels_joined, definitions=definitions_string, column_1=example[0], column_2=example[1])\n",
    "\n",
    "            prompts.append(text_prompt)\n",
    "\n",
    "            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=100)\n",
    "            model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/cpa-manual-definitions-prompt-column-0-shot.pkl\", model_answers)\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/cpa-manual-definitions-prompt-column-0-shot-prompts.pkl\", prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d6fdf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for syst in syst_prompts:\n",
    "    for g in a_prompts:\n",
    "        if f\"cpa-{syst}_{g}-column-0-shot.pkl\" in os.listdir(f\"predictions/{dataset}/{model}/\"):\n",
    "            preds = load_pickle_file(f\"predictions/{dataset}/{model}/cpa-{syst}_{g}-column-0-shot.pkl\")\n",
    "            prompts = load_pickle_file(f\"predictions/{dataset}/{model}/cpa-{syst}_{g}-column-0-shot-prompts.pkl\")\n",
    "\n",
    "            predictions, num = map_answers_column(preds,prompts)\n",
    "\n",
    "            types = list(set(labels))\n",
    "            types = types + [\"-\"] if \"-\" in predictions else types\n",
    "            evaluation, per_class_eval = calculate_f1_scores(labels, predictions, len(types), types)\n",
    "\n",
    "            print(f\"{syst}_{g}\\t{decimal(evaluation['Precision'])}\\t{decimal(evaluation['Recall'])}\\t{decimal(evaluation['Macro-F1'])}\\t{decimal(evaluation['Micro-F1'])}\\t{num}\")  \n",
    "\n",
    "for tb in tb_prompts:\n",
    "    for syst in syst_prompts[:-1]: # skip last system for prompt b\n",
    "        for inst in inst_prompts:\n",
    "            if f\"cpa-{syst}_{inst}_{tb}-column-0-shot.pkl\" in os.listdir(f\"predictions/{dataset}/{model}/\"):\n",
    "                preds = load_pickle_file(f\"predictions/{dataset}/{model}/cpa-{syst}_{inst}_{tb}-column-0-shot.pkl\")\n",
    "                prompts = load_pickle_file(f\"predictions/{dataset}/{model}/cpa-{syst}_{inst}_{tb}-column-0-shot-prompts.pkl\")\n",
    "\n",
    "                predictions, num = map_answers_column(preds,prompts)\n",
    "\n",
    "                types = list(set(labels))\n",
    "                types = types + [\"-\"] if \"-\" in predictions else types\n",
    "                evaluation, per_class_eval = calculate_f1_scores(labels, predictions, len(types), types)\n",
    "\n",
    "                print(f\"{syst}_{inst}_{tb}\\t{decimal(evaluation['Precision'])}\\t{decimal(evaluation['Recall'])}\\t{decimal(evaluation['Macro-F1'])}\\t{decimal(evaluation['Micro-F1'])}\\t{num}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110350ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
