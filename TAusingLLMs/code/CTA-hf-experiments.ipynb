{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed23b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from langchain import PromptTemplate\n",
    "from utils import load_pickle_file, load_cta_dataset_column, save_pickle_file, load_cta_dataset, calculate_f1_scores, decimal, map_answers_column, map_cta_labels, map_sportstables\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\" #4,5,6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"sotabv2\", \"t2dv2-webtables\", \"sportstables\"]\n",
    "\n",
    "# StableBeluga7B\n",
    "model_name = \"stabilityai/StableBeluga-7B\"\n",
    "mod = \"stablebeluga7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"hf_cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", cache_dir=\"hf_cache/\")\n",
    "\n",
    "# SOLAR\n",
    "# model_name = \"upstage/SOLAR-0-70b-16bit\"\n",
    "# mod = \"solar\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"hf_cache/\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, load_in_8bit=True, device_map=\"auto\", cache_dir=\"hf_cache/\", temperature=0, do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e8620",
   "metadata": {},
   "source": [
    "## Table-prompts experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5bfa86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_template = \"\"\"Answer the question based on the task and instructions below.\n",
    "Task: Classify the columns of a given table with only one of the following classes that are separated with comma: {labels_joined}.\n",
    "Instructions: 1. Look at the input given to you and make a table out of it. 2. Look at the cell values in detail. 3. For each column, select a class that best represents the meaning of all cells in the column. 4. Answer with the selected class for each columns with the format Column1: class. 5. Answer only with labels from the provided label set!\n",
    "Table:\n",
    "{input_string}\n",
    "Class:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "111fa01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_template = \"\"\"Answer the question based on the task and instructions below.\n",
    "Task: Classify the columns of a given table with only one of the following classes that are separated with comma: {labels_joined}.\n",
    "Instructions: 1. Look at the input given to you and make a table out of it. 2. Look at the cell values in detail. 3. For each column, select a class that best represents the meaning of all cells in the column. 4. Answer with the selected class for each columns with the format Column1: class. 5. Answer only with labels from the provided label set!\n",
    "{examples}\n",
    "Table:\n",
    "{input_string}Class:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2874ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    # Load dataset\n",
    "    examples, labels, test_table_type_labels, train_examples, train_example_labels, train_table_type_labels, labels_to_text, text_to_label, labels_joined, train, test = load_cta_dataset(dataset,\"\")\n",
    "    all_labels = [labels_to_text[l] for l in labels_to_text]\n",
    "\n",
    "    examples_demonstrations = load_pickle_file(f\"embeddings/examples_demonstrations_{dataset}.pkl\")\n",
    "    cc_demonstrations = load_pickle_file(f\"embeddings/cc_examples_demonstrations_{dataset}.pkl\")\n",
    "\n",
    "    # Zero-shot\n",
    "    prompt = PromptTemplate(template=zero_template, input_variables=['input_string', 'labels_joined'])\n",
    "            \n",
    "    prompts = []\n",
    "    model_answers = []\n",
    "            \n",
    "    for example in tqdm.tqdm(examples, total=len(examples)):\n",
    "        text_prompt = prompt.format(input_string=example.strip(), labels_joined=labels_joined)\n",
    "        prompts.append(text_prompt)\n",
    "\n",
    "        inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "        model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-table-0-shot.pkl\", model_answers)\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-table-0-shot-prompts.pkl\", prompts)\n",
    "\n",
    "    # Few-shot\n",
    "    for j in [1, 3]:\n",
    "        prompts = []\n",
    "        model_answers = []\n",
    "        prompt = PromptTemplate(template=few_template, input_variables=['input_string', 'examples', 'labels_joined'])\n",
    "\n",
    "        for example in tqdm.tqdm(examples, total=len(examples)):\n",
    "\n",
    "            random_examples = \"\"\"\"\"\"\n",
    "            for i in range(0,j):\n",
    "                index = random.randint(0, len(train_examples)-1)\n",
    "                random_examples += f\"\"\"Table:\\n{train_examples[index]}Class:\\n{train_example_labels[index]}\\n\"\"\"\n",
    "            random_examples = random_examples.strip()\n",
    "\n",
    "            text_prompt = prompt.format(input_string=example, examples=random_examples, labels_joined=labels_joined)\n",
    "            prompts.append(text_prompt)\n",
    "\n",
    "            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "            model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "            \n",
    "        save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-table-{j}-shot.pkl\", model_answers)\n",
    "        save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-table-{j}-shot-prompts.pkl\", prompts)\n",
    "\n",
    "    # Few-shot: similar\n",
    "    prompts = []\n",
    "    model_answers = []\n",
    "    prompt = PromptTemplate(template=few_template, input_variables=['input_string', 'examples', 'labels_joined'])\n",
    "\n",
    "    for i, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "\n",
    "        random_examples = \"\"\"\"\"\"\n",
    "        for index in examples_demonstrations[i][-3:]:\n",
    "            random_examples += f\"\"\"Table:\\n{train_examples[index]}Class:\\n{train_example_labels[index]}\\n\"\"\"    \n",
    "        random_examples = random_examples.strip()\n",
    "\n",
    "        text_prompt = prompt.format(input_string=example, examples=random_examples, labels_joined=labels_joined)\n",
    "        prompts.append(text_prompt)\n",
    "\n",
    "        inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "        model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-table-3-similar-shot.pkl\", model_answers)\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-table-3-similar-shot-prompts.pkl\", prompts)\n",
    "    \n",
    "    # Few-shot: corner-cases\n",
    "    prompts = []\n",
    "    model_answers = []\n",
    "    prompt = PromptTemplate(template=few_template, input_variables=['input_string', 'examples', 'labels_joined'])\n",
    "\n",
    "    for i, example in tqdm.tqdm(enumerate(examples), total=len(examples)):\n",
    "\n",
    "        random_examples = \"\"\"\"\"\"\n",
    "        for index in cc_demonstrations[i]:\n",
    "            random_examples += f\"\"\"Table:\\n{train_examples[index]}Class:\\n{train_example_labels[index]}\\n\"\"\"    \n",
    "        random_examples = random_examples.strip()\n",
    "\n",
    "        text_prompt = prompt.format(input_string=example, examples=random_examples, labels_joined=labels_joined)\n",
    "        prompts.append(text_prompt)\n",
    "\n",
    "        inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "        model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-table-4-cc-shot.pkl\", model_answers)\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-table-4-cc-shot-prompts.pkl\", prompts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d802f92",
   "metadata": {},
   "source": [
    "## Column-prompts experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_template = \"\"\"Answer the question based on the task and instructions below.\n",
    "Task: Classify the column given to you into only one of these classes that are separated with comma: {labels_joined}.\n",
    "Instructions: 1. Look at the column and the classes given to you. 2. Examine the values of the column. 3. Select a class that best represents the meaning of the column. 4. Answer with the selected class.\n",
    "Column: {input_string}\n",
    "Class:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_template = \"\"\"Answer the question based on the task and instructions below.\n",
    "Task: Classify the column given to you into only one of these classes that are separated with comma: {labels_joined}.\n",
    "Instructions: 1. Look at the column and the classes given to you. 2. Examine the values of the column. 3. Select a class that best represents the meaning of the column. 4. Answer with the selected class.\n",
    "{examples}\n",
    "Column: {input_string}\n",
    "Class:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae08f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    # Load dataset\n",
    "    examples, labels, train_examples, train_labels, labels_to_text, text_to_label, labels_joined, train, test = load_cta_dataset_column(dataset,\"\")\n",
    "    all_labels = [labels_to_text[l] for l in labels_to_text]\n",
    "\n",
    "    examples_demonstrations = load_pickle_file(f\"embeddings/examples_demonstrations_{dataset}-column.pkl\")\n",
    "\n",
    "    # Zero-shot\n",
    "    prompt = PromptTemplate(template=zero_template, input_variables=['input_string', 'labels_joined'])\n",
    "        \n",
    "    prompts = []\n",
    "    model_answers = [] \n",
    "\n",
    "    for example in tqdm.tqdm(examples, total=len(examples)):\n",
    "        text_prompt = prompt.format(input_string=example.strip(), labels_joined=labels_joined)\n",
    "        prompts.append(text_prompt) \n",
    "        \n",
    "        inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "        model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))        \n",
    "\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-column-0-shot.pkl\", model_answers)\n",
    "    save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-column-0-shot-prompts.pkl\", prompts)\n",
    "\n",
    "    for j in [1,3]:\n",
    "        prompts = []\n",
    "        model_answers = []\n",
    "\n",
    "        prompt = PromptTemplate(template=few_template, input_variables=['input_string', 'examples', 'labels_joined'])\n",
    "\n",
    "        for example in tqdm.tqdm(examples, total=len(examples)):\n",
    "\n",
    "            random_examples = \"\"\"\"\"\"\n",
    "            for i in range(0,j):\n",
    "                index = random.randint(0, len(train_examples)-1)\n",
    "                random_examples += f\"\"\"Column: {train_examples[index]}\\nClass: {train_labels[index]}\\n\"\"\"\n",
    "            random_examples = random_examples.strip()\n",
    "\n",
    "            text_prompt = prompt.format(input_string=example, examples=random_examples, labels_joined=labels_joined)\n",
    "            prompts.append(text_prompt)\n",
    "\n",
    "            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "            model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "        save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-column-{j}-shot.pkl\", model_answers)\n",
    "        save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-column-{j}-shot-prompts.pkl\", prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8880967",
   "metadata": {},
   "source": [
    "## Two-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6c66244",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_template = \"\"\"Your task is to classify a table into one of these domains: {domains_list}.\n",
    "Your instructions are: 1. Look at the input given to you and make a table out of it. 2. Look at the cell values in detail. 3. Decide the domain that best represents the table. 4. Answer with one domain.\n",
    "{examples}\n",
    "Classify this table: {input_string}\n",
    "Answer:\"\"\"\n",
    "\n",
    "template = \"\"\"Answer the question based on the task and instructions below.\n",
    "Task: Classify the columns of a given table with only one of the following classes that are separated with comma: {labels_joined}.\n",
    "Instructions: 1. Look at the input given to you and make a table out of it. 2. Look at the cell values in detail. 3. For each column, select a class that best represents the meaning of all cells in the column. 4. Answer with the selected class for each columns with the format Column1: class. 5. Answer only with labels from the provided label set!\n",
    "{examples}\n",
    "Table:\n",
    "{input_string}\n",
    "Class:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c62ebf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_table_prediction(table_pred, domains):\n",
    "    cleaned_table_pred=\"-\"\n",
    "    for dom in domains:\n",
    "        if dom in table_pred:\n",
    "            cleaned_table_pred = dom\n",
    "            break\n",
    "    return cleaned_table_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aef69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    # Load dataset\n",
    "    examples, labels, test_table_type_labels, train_examples, train_example_labels, train_table_type_labels, labels_to_text, text_to_label, labels_joined, train, test = load_cta_dataset(dataset,\"\")\n",
    "    all_labels = [labels_to_text[l] for l in labels_to_text]\n",
    "\n",
    "    domains = set(train_table_type_labels)\n",
    "    labels_dict = {}\n",
    "    for dom in domains:\n",
    "        f = open(f\"../data/sportstables-labels/{dataset}_{dom}_labels.txt\", 'r')\n",
    "        t = [line.split('\\n')[0] for line in f.readlines()]\n",
    "        labels_dict[dom] = t\n",
    "\n",
    "    domains_list = \", \".join(domains)\n",
    "\n",
    "    for j in [0, 1, 3]:\n",
    "        # Step 1\n",
    "        table_prompts = []\n",
    "        table_model_answers = []\n",
    "        # Step 2\n",
    "        prompts = []\n",
    "        model_answers = []\n",
    "\n",
    "        for example in tqdm.tqdm(examples, total=len(examples)):\n",
    "            prompt = PromptTemplate(template=table_template, input_variables=['input_string', 'domains_list', 'examples'])\n",
    "\n",
    "            random_examples = \"\"\"\"\"\"\n",
    "\n",
    "            for i in range(0,j):\n",
    "                index = random.randint(0, len(train_examples)-1)\n",
    "                random_examples += f\"\"\"Classify this table:{train_examples[index]}Class: {train_table_type_labels[index]}\\n\"\"\"\n",
    "\n",
    "            random_examples = random_examples.strip()\n",
    "\n",
    "            text_prompt = prompt.format(input_string=example, examples=random_examples, domains_list=domains_list)\n",
    "            table_prompts.append(text_prompt)\n",
    "\n",
    "            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "            answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            table_model_answers.append(answer)\n",
    "\n",
    "            clean_prediction = get_clean_table_prediction(answer.replace(text_prompt, \"\").strip(), domains)\n",
    "\n",
    "            prompt = PromptTemplate(template=template, input_variables=['input_string','labels_joined', 'examples'])\n",
    "\n",
    "            random_examples = \"\"\"\"\"\"\n",
    "\n",
    "            if clean_prediction != \"-\":\n",
    "                labels_dom = \", \".join([labels_to_text[l] for l in labels_dict[clean_prediction]])\n",
    "\n",
    "                for m in range(0,j):\n",
    "                    index = random.choice([j for j, e in enumerate(train_table_type_labels) if e == clean_prediction])\n",
    "                    random_examples += f\"\"\"Table:\\n{train_examples[index]}Class:\\n{train_example_labels[index]}\\n\"\"\"\n",
    "                random_examples = random_examples.strip()\n",
    "\n",
    "            else:\n",
    "                labels_dom = labels_joined\n",
    "\n",
    "            text_prompt = prompt.format(input_string=example, examples=random_examples, labels_joined=labels_dom)\n",
    "            prompts.append(text_prompt)\n",
    "\n",
    "            inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "            model_answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "        save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-two-step-{j}-shot-step1.pkl\", table_model_answers)\n",
    "        save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-two-step-{j}-shot-step1-prompts.pkl\", table_prompts)\n",
    "        save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-two-step-{j}-shot-step2.pkl\", model_answers)\n",
    "        save_pickle_file(f\"predictions/{dataset}/{mod}/prompt-two-step-{j}-shot-step2-prompts.pkl\", prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6a291",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column prompt evaluation\n",
    "print(f\"\\tPrecision)\\tRecall\\tMacro-F1\\tMicro-F1\\tOOV\")\n",
    "for nr in [0, 1, 3]:\n",
    "    preds = load_pickle_file(f'predictions/{dataset}/{model}/prompt-column-{nr}-shot.pkl')\n",
    "    prompts = load_pickle_file(f'predictions/{dataset}/{model}/prompt-column-{nr}-shot-prompts.pkl')\n",
    "\n",
    "    preds = [pred.replace(prompts[i], \"\") for i,pred in enumerate(preds)]\n",
    "    \n",
    "    predictions, num = map_answers_column(preds,prompts)\n",
    "    types = list(set(labels))\n",
    "    types = types + [\"-\"] if \"-\" in predictions else types\n",
    "    evaluation, per_class_eval = calculate_f1_scores(labels, predictions, len(types), types)\n",
    "    \n",
    "    print(f\"{decimal(evaluation['Precision'])}\\t{decimal(evaluation['Recall'])}\\t{decimal(evaluation['Macro-F1'])}\\t{decimal(evaluation['Micro-F1'])}\\t{num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15110667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table prompt evaluation\n",
    "print(f\"\\tPrecision)\\tRecall\\tMacro-F1\\tMicro-F1\\tOOV\")\n",
    "for nr in [ 0, 1, 3, \"3-similar\", \"4-cc\"]:\n",
    "    preds = load_pickle_file(f'icde-predictions/{dataset}/{model}/prompt-table-{nr}-shot.pkl')\n",
    "    prompts = load_pickle_file(f'icde-predictions/{dataset}/{model}/prompt-table-{nr}-shot-prompts.pkl')\n",
    "\n",
    "    preds = [pred.replace(prompts[i], \"\") for i,pred in enumerate(preds)]\n",
    "\n",
    "    predictions, num = map_cta_labels(preds,prompts)\n",
    "    types = list(set(labels))\n",
    "    types = types + [\"-\"] if \"-\" in predictions else types\n",
    "    evaluation, per_class_eval = calculate_f1_scores(labels, predictions, len(types), types)\n",
    "    \n",
    "    print(f\"{decimal(evaluation['Precision'])}\\t{decimal(evaluation['Recall'])}\\t{decimal(evaluation['Macro-F1'])}\\t{decimal(evaluation['Micro-F1'])}\\t{num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SportsTables table prompt evaluation\n",
    "print(f\"\\tPrecision)\\tRecall\\tMacro-F1\\tMicro-F1\\tOOV\")\n",
    "for nr in [ 0, 1, 3, \"3-similar\", \"4-cc\"]:\n",
    "    preds = load_pickle_file(f'icde-predictions/{dataset}/{model}/prompt-table-{nr}-shot.pkl')\n",
    "    prompts = load_pickle_file(f'icde-predictions/{dataset}/{model}/prompt-table-{nr}-shot-prompts.pkl')\n",
    "    preds = [pred.replace(prompts[i], \"\") for i,pred in enumerate(preds)]\n",
    "\n",
    "    labels = [l for l in labels if l!=\"\"]\n",
    "    predictions, num = map_sportstables(preds,prompts)\n",
    "    \n",
    "    types = list(set(labels))\n",
    "    types = types + [\"-\"] if \"-\" in predictions else types\n",
    "    evaluation, per_class_eval = calculate_f1_scores(labels, predictions, len(types), types)\n",
    "    \n",
    "    print(f\"{decimal(evaluation['Precision'])}\\t{decimal(evaluation['Recall'])}\\t{decimal(evaluation['Macro-F1'])}\\t{decimal(evaluation['Micro-F1'])}\\t{num}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
